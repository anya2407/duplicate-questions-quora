{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2f928eb-4522-4291-9bec-6e7b09ef1cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              0\n",
       "qid1            0\n",
       "qid2            0\n",
       "question1       0\n",
       "question2       0\n",
       "is_duplicate    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r\"C:\\Users\\anyaa\\Documents\\nlp\\quora\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "from scipy.sparse import hstack,vstack\n",
    "import contractions\n",
    "df=pd.read_csv(\"questions.csv\")\n",
    "df.head(3)\n",
    "df=df.dropna()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3b49f20-3008-4758-940d-38f4fe089783",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lowercase (vectorized → fast)\n",
    "df['question1'] = df['question1'].str.lower()\n",
    "df['question2'] = df['question2'].str.lower()\n",
    "# Contractions (still needs apply, but we optimize usage)\n",
    "df['question1'] = df['question1'].map(contractions.fix)\n",
    "df['question2'] = df['question2'].map(contractions.fix)\n",
    "# Remove special characters (vectorized → fast)\n",
    "df['question1'] = df['question1'].str.replace(r\"[^a-zA-Z0-9?!\\s]\", \" \", regex=True)\n",
    "df['question2'] = df['question2'].str.replace(r\"[^a-zA-Z0-9?!\\s]\", \" \", regex=True)\n",
    "# Remove extra spaces (vectorized → fast)\n",
    "df['question1'] = df['question1'].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "df['question2'] = df['question2'].str.replace(r\"\\s+\", \" \", regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73be7674-da20-4b94-a451-ddf322a10764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[['question1','question2']],\n",
    "    df['is_duplicate'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adc649c6-3cc2-4554-886c-a226bc3ada83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\",\"ner\"])\n",
    "def lemmatize_series(series):\n",
    "    return [\n",
    "        \" \".join([token.lemma_ for token in doc])\n",
    "        for doc in nlp.pipe(series, batch_size=500)\n",
    "    ]\n",
    "\n",
    "X_train['question1'] = lemmatize_series(X_train['question1'])\n",
    "X_train['question2'] = lemmatize_series(X_train['question2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28093683-6164-4cbd-bbfa-30aa5a534a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute char lengths once\n",
    "l1 = X_train['question1'].str.len()\n",
    "l2 = X_train['question2'].str.len()\n",
    "\n",
    "# Assign\n",
    "X_train['lencq1'] = l1\n",
    "X_train['lencq2'] = l2\n",
    "X_train['lendiff'] = l1 - l2\n",
    "\n",
    "# Word lengths\n",
    "X_train['lenq1'] = X_train['question1'].str.split().str.len()\n",
    "X_train['lenq2'] = X_train['question2'].str.split().str.len()\n",
    "\n",
    "#common words\n",
    "q1_words = X_train['question1'].str.split()\n",
    "q2_words = X_train['question2'].str.split()\n",
    "X_train['common_word_count'] = [\n",
    "    len(set(a) & set(b)) for a,b in zip(q1_words, q2_words)\n",
    "]\n",
    "#jaccard similarity\n",
    "def jaccard(q1, q2):\n",
    "    w1 = set(str(q1).split())\n",
    "    w2 = set(str(q2).split())\n",
    "    if len(w1.union(w2)) == 0:\n",
    "        return 0\n",
    "    return len(w1.intersection(w2)) / len(w1.union(w2))\n",
    "\n",
    "X_train['jaccard'] = [\n",
    "    len(set(a.split()) & set(b.split())) /\n",
    "    len(set(a.split()) | set(b.split()))\n",
    "    if len(set(a.split()) | set(b.split())) > 0 else 0\n",
    "    for a,b in zip(X_train['question1'], X_train['question2'])\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cf99d55-808d-4b1e-a746-7a6c7887ba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=20000,ngram_range=(1,2),min_df=2)\n",
    "\n",
    "tfidf.fit(X_train['question1'].tolist()+X_train['question2'].tolist())\n",
    "q1 = tfidf.transform(X_train['question1'])\n",
    "q2 = tfidf.transform(X_train['question2'])\n",
    "\n",
    "X_tfidf_train = hstack([q1, q2])\n",
    "\n",
    "train_cosine = q1.multiply(q2).sum(axis=1).A1\n",
    "X_train['cosine_sim'] = train_cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9490c262-0e55-44e7-b6fb-d70d10fddaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    }
   ],
   "source": [
    "#word to vec\n",
    "from gensim.models import Word2Vec\n",
    "sentences = (\n",
    "    X_train['question1'].str.split().tolist() +\n",
    "    X_train['question2'].str.split().tolist()\n",
    ")\n",
    "w2v = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4\n",
    ")\n",
    "import numpy as np\n",
    "def sent_vec(text):\n",
    "    words = text.split()\n",
    "    vecs = [w2v.wv[w] for w in words if w in w2v.wv]\n",
    "    \n",
    "    if len(vecs) == 0:\n",
    "        return np.zeros(100)\n",
    "        \n",
    "    return np.mean(vecs, axis=0)\n",
    "q1_vec_train = np.vstack(X_train['question1'].apply(sent_vec))\n",
    "q2_vec_train = np.vstack(X_train['question2'].apply(sent_vec))\n",
    "#X_train_w2v = np.hstack([q1_vec_train, q2_vec_train])\n",
    "\n",
    "from numpy.linalg import norm\n",
    "\n",
    "w2v_cos_train = (\n",
    "    np.sum(q1_vec_train * q2_vec_train, axis=1) /\n",
    "    (norm(q1_vec_train, axis=1) * norm(q2_vec_train, axis=1) + 1e-9)\n",
    ")\n",
    "\n",
    "X_train['w2v_cosine'] = w2v_cos_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61bfad21-fce2-4b01-950c-66d588183f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "numeric_cols = ['lenq1','lenq2','lencq1','lencq2','lendiff','common_word_count','jaccard','cosine_sim','w2v_cosine']\n",
    "num_train = csr_matrix(X_train[numeric_cols].to_numpy())\n",
    "\n",
    "X_final_train = hstack([X_tfidf_train, num_train])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3840b23a-26d0-4a1f-8f6e-9004c12730b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test['question1'] = lemmatize_series(X_test['question1'])\n",
    "X_test['question2'] = lemmatize_series(X_test['question2'])\n",
    "    \n",
    "q1_vec_test = np.vstack(X_test['question1'].apply(sent_vec))\n",
    "q2_vec_test = np.vstack(X_test['question2'].apply(sent_vec))\n",
    "\n",
    "w2v_cos_test = (\n",
    "    np.sum(q1_vec_test * q2_vec_test, axis=1) /\n",
    "    (norm(q1_vec_test, axis=1) * norm(q2_vec_test, axis=1) + 1e-9)\n",
    ")\n",
    "X_test['w2v_cosine'] = w2v_cos_test\n",
    "\n",
    "#X_test_w2v = np.hstack([q1_vec_test, q2_vec_test])\n",
    "\n",
    "q1_test = tfidf.transform(X_test['question1'])\n",
    "q2_test = tfidf.transform(X_test['question2'])\n",
    "\n",
    "\n",
    "X_test['lenq1'] = X_test['question1'].str.split().str.len()\n",
    "X_test['lenq2'] = X_test['question2'].str.split().str.len()\n",
    "\n",
    "X_test['lencq1']=X_test['question1'].str.len()\n",
    "X_test['lencq2']=X_test['question2'].str.len()\n",
    "X_test['lendiff']=X_test['question1'].str.len()-X_test['question2'].str.len()\n",
    "def common_words(q1, q2):\n",
    "    w1 = set(str(q1).lower().split())\n",
    "    w2 = set(str(q2).lower().split())\n",
    "    return len(w1.intersection(w2))\n",
    "X_test['common_word_count'] = [\n",
    "    len(set(a.split()) & set(b.split()))\n",
    "    for a,b in zip(X_test['question1'], X_test['question2'])\n",
    "]\n",
    "\n",
    "def jaccard(q1, q2):\n",
    "    w1 = set(str(q1).lower().split())\n",
    "    w2 = set(str(q2).lower().split())\n",
    "    if len(w1.union(w2)) == 0:\n",
    "        return 0\n",
    "    return len(w1.intersection(w2)) / len(w1.union(w2))\n",
    "X_test['jaccard'] = X_test.apply(\n",
    "    lambda row: jaccard(row['question1'], row['question2']),\n",
    "    axis=1\n",
    ")\n",
    "test_cosine = q1_test.multiply(q2_test).sum(axis=1).A1\n",
    "X_test['cosine_sim'] = test_cosine\n",
    "numeric_cols = [\n",
    "    'lenq1',\n",
    "    'lenq2',\n",
    "    'lencq1','lencq2','lendiff',\n",
    "    'common_word_count',\n",
    "    'jaccard',\n",
    "    'cosine_sim',\n",
    "    'w2v_cosine'\n",
    "]\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "num_test_sparse = csr_matrix(X_test[numeric_cols].values)\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "X_tfidf_test = hstack([q1_test, q2_test])\n",
    "X_final_test = hstack([X_tfidf_test, num_test_sparse])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8ae443b-2d3b-48a3-affa-c988b8968f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 119676, number of negative: 203802\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 11.840552 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1035822\n",
      "[LightGBM] [Info] Number of data points in the train set: 323478, number of used features: 39276\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.369966 -> initscore=-0.532361\n",
      "[LightGBM] [Info] Start training from score -0.532361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8139359465809324\n",
      "F1: 0.7399906689015223\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "model = LGBMClassifier(\n",
    "    n_estimators=400,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=63,\n",
    "    max_depth=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(X_final_train, y_train)\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "y_pred = model.predict(X_final_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1:\", f1_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372e914d-db97-4455-93c8-9dae5a5e574c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(C=1.2,solver=\"saga\",max_iter=2200,class_weight=\"balanced\")\n",
    "model.fit(X_final_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f96d50-bc78-4c7c-9b43-e6d6c22f060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "\n",
    "X_final_train = imputer.fit_transform(X_final_train)\n",
    "X_final_test = imputer.transform(X_final_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f249efaf-5c52-4758-8a48-3cf13b849133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "y_pred = model.predict(X_final_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1:\", f1_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fa1100-46f3-48b0-b534-6c977bb4f54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "feature_importance = np.abs(model.coef_[0])\n",
    "for col, imp in zip(numeric_cols, feature_importance[-len(numeric_cols):]):\n",
    "    print(col, imp)\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "tfidf_weights = feature_importance[:len(feature_names)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed40f5ed-c58a-4100-821c-03bca3e7bb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scores = cross_validate(\n",
    "    model,\n",
    "    X_final_train,\n",
    "    y_train,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"Train F1:\", scores['train_score'].mean())\n",
    "print(\"Val F1:\", scores['test_score'].mean())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
