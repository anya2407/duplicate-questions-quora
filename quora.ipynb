{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2f928eb-4522-4291-9bec-6e7b09ef1cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              0\n",
       "qid1            0\n",
       "qid2            0\n",
       "question1       0\n",
       "question2       0\n",
       "is_duplicate    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r\"C:\\Users\\anyaa\\Documents\\nlp\\quora\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "from scipy.sparse import hstack,vstack\n",
    "import contractions\n",
    "df=pd.read_csv(\"questions.csv\")\n",
    "df.head(3)\n",
    "df=df.dropna()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3b49f20-3008-4758-940d-38f4fe089783",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lowercase (vectorized → fast)\n",
    "df['question1'] = df['question1'].str.lower()\n",
    "df['question2'] = df['question2'].str.lower()\n",
    "# Contractions (still needs apply, but we optimize usage)\n",
    "df['question1'] = df['question1'].map(contractions.fix)\n",
    "df['question2'] = df['question2'].map(contractions.fix)\n",
    "# Remove special characters (vectorized → fast)\n",
    "df['question1'] = df['question1'].str.replace(r\"[^a-zA-Z0-9?!\\s]\", \" \", regex=True)\n",
    "df['question2'] = df['question2'].str.replace(r\"[^a-zA-Z0-9?!\\s]\", \" \", regex=True)\n",
    "# Remove extra spaces (vectorized → fast)\n",
    "df['question1'] = df['question1'].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "df['question2'] = df['question2'].str.replace(r\"\\s+\", \" \", regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73be7674-da20-4b94-a451-ddf322a10764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[['question1','question2']],\n",
    "    df['is_duplicate'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adc649c6-3cc2-4554-886c-a226bc3ada83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\",\"ner\"])\n",
    "def lemmatize_series(series):\n",
    "    return [\n",
    "        \" \".join([token.lemma_ for token in doc])\n",
    "        for doc in nlp.pipe(series, batch_size=500)\n",
    "    ]\n",
    "\n",
    "X_train['question1'] = lemmatize_series(X_train['question1'])\n",
    "X_train['question2'] = lemmatize_series(X_train['question2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28093683-6164-4cbd-bbfa-30aa5a534a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute char lengths once\n",
    "l1 = X_train['question1'].str.len()\n",
    "l2 = X_train['question2'].str.len()\n",
    "\n",
    "# Assign\n",
    "X_train['lencq1'] = l1\n",
    "X_train['lencq2'] = l2\n",
    "X_train['lendiff'] = l1 - l2\n",
    "\n",
    "# Word lengths\n",
    "X_train['lenq1'] = X_train['question1'].str.split().str.len()\n",
    "X_train['lenq2'] = X_train['question2'].str.split().str.len()\n",
    "\n",
    "#common words\n",
    "q1_words = X_train['question1'].str.split()\n",
    "q2_words = X_train['question2'].str.split()\n",
    "X_train['common_word_count'] = [\n",
    "    len(set(a) & set(b)) for a,b in zip(q1_words, q2_words)\n",
    "]\n",
    "#jaccard similarity\n",
    "def jaccard(q1, q2):\n",
    "    w1 = set(str(q1).split())\n",
    "    w2 = set(str(q2).split())\n",
    "    if len(w1.union(w2)) == 0:\n",
    "        return 0\n",
    "    return len(w1.intersection(w2)) / len(w1.union(w2))\n",
    "\n",
    "X_train['jaccard'] = [\n",
    "    len(set(a.split()) & set(b.split())) /\n",
    "    len(set(a.split()) | set(b.split()))\n",
    "    if len(set(a.split()) | set(b.split())) > 0 else 0\n",
    "    for a,b in zip(X_train['question1'], X_train['question2'])\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cf99d55-808d-4b1e-a746-7a6c7887ba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=20000,ngram_range=(1,2),min_df=2)\n",
    "\n",
    "tfidf.fit(X_train['question1'].tolist()+X_train['question2'].tolist())\n",
    "q1 = tfidf.transform(X_train['question1'])\n",
    "q2 = tfidf.transform(X_train['question2'])\n",
    "\n",
    "X_tfidf_train = hstack([q1, q2])\n",
    "\n",
    "train_cosine = q1.multiply(q2).sum(axis=1).A1\n",
    "X_train['cosine_sim'] = train_cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9490c262-0e55-44e7-b6fb-d70d10fddaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    }
   ],
   "source": [
    "#word to vec\n",
    "from gensim.models import Word2Vec\n",
    "sentences = (\n",
    "    X_train['question1'].str.split().tolist() +\n",
    "    X_train['question2'].str.split().tolist()\n",
    ")\n",
    "w2v = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4\n",
    ")\n",
    "import numpy as np\n",
    "def sent_vec(text):\n",
    "    words = text.split()\n",
    "    vecs = [w2v.wv[w] for w in words if w in w2v.wv]\n",
    "    \n",
    "    if len(vecs) == 0:\n",
    "        return np.zeros(100)\n",
    "        \n",
    "    return np.mean(vecs, axis=0)\n",
    "q1_vec_train = np.vstack(X_train['question1'].apply(sent_vec))\n",
    "q2_vec_train = np.vstack(X_train['question2'].apply(sent_vec))\n",
    "X_train_w2v = np.hstack([q1_vec_train, q2_vec_train])\n",
    "\n",
    "from numpy.linalg import norm\n",
    "\n",
    "w2v_cos_train = (\n",
    "    np.sum(q1_vec_train * q2_vec_train, axis=1) /\n",
    "    (norm(q1_vec_train, axis=1) * norm(q2_vec_train, axis=1) + 1e-9)\n",
    ")\n",
    "\n",
    "X_train['w2v_cosine'] = w2v_cos_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9c02b42-a088-4085-9a2b-a908063bb9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "numeric_cols = ['lenq1','lenq2','lencq1','lencq2','lendiff','common_word_count','jaccard','cosine_sim','w2v_cosine']\n",
    "num_train = csr_matrix(X_train[numeric_cols].to_numpy())\n",
    "\n",
    "X_final_train = hstack([X_tfidf_train, num_train, X_train_w2v])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3840b23a-26d0-4a1f-8f6e-9004c12730b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test['question1'] = lemmatize_series(X_test['question1'])\n",
    "X_test['question2'] = lemmatize_series(X_test['question2'])\n",
    "    \n",
    "q1_vec_test = np.vstack(X_test['question1'].apply(sent_vec))\n",
    "q2_vec_test = np.vstack(X_test['question2'].apply(sent_vec))\n",
    "\n",
    "w2v_cos_test = (\n",
    "    np.sum(q1_vec_test * q2_vec_test, axis=1) /a\n",
    "    (norm(q1_vec_test, axis=1) * norm(q2_vec_test, axis=1) + 1e-9)\n",
    ")\n",
    "X_test['w2v_cosine'] = w2v_cos_test\n",
    "\n",
    "X_test_w2v = np.hstack([q1_vec_test, q2_vec_test])\n",
    "\n",
    "q1_test = tfidf.transform(X_test['question1'])\n",
    "q2_test = tfidf.transform(X_test['question2'])\n",
    "\n",
    "\n",
    "X_test['lenq1'] = X_test['question1'].str.split().str.len()\n",
    "X_test['lenq2'] = X_test['question2'].str.split().str.len()\n",
    "\n",
    "X_test['lencq1']=X_test['question1'].str.len()\n",
    "X_test['lencq2']=X_test['question2'].str.len()\n",
    "X_test['lendiff']=X_test['question1'].str.len()-X_test['question2'].str.len()\n",
    "def common_words(q1, q2):\n",
    "    w1 = set(str(q1).lower().split())\n",
    "    w2 = set(str(q2).lower().split())\n",
    "    return len(w1.intersection(w2))\n",
    "X_test['common_word_count'] = [\n",
    "    len(set(a.split()) & set(b.split()))\n",
    "    for a,b in zip(X_test['question1'], X_test['question2'])\n",
    "]\n",
    "\n",
    "def jaccard(q1, q2):\n",
    "    w1 = set(str(q1).lower().split())\n",
    "    w2 = set(str(q2).lower().split())\n",
    "    if len(w1.union(w2)) == 0:\n",
    "        return 0\n",
    "    return len(w1.intersection(w2)) / len(w1.union(w2))\n",
    "X_test['jaccard'] = X_test.apply(\n",
    "    lambda row: jaccard(row['question1'], row['question2']),\n",
    "    axis=1\n",
    ")\n",
    "test_cosine = q1_test.multiply(q2_test).sum(axis=1).A1\n",
    "X_test['cosine_sim'] = test_cosine\n",
    "numeric_cols = [\n",
    "    'lenq1',\n",
    "    'lenq2',\n",
    "    'lencq1','lencq2','lendiff',\n",
    "    'common_word_count',\n",
    "    'jaccard',\n",
    "    'cosine_sim',\n",
    "    'w2v_cosine'\n",
    "]\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "num_test_sparse = csr_matrix(X_test[numeric_cols].values)\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "X_tfidf_test = hstack([q1_test, q2_test])\n",
    "X_final_test = hstack([X_tfidf_test, num_test_sparse,X_test_w2v])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8ae443b-2d3b-48a3-affa-c988b8968f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 119676, number of negative: 203802\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 14.068276 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1086822\n",
      "[LightGBM] [Info] Number of data points in the train set: 323478, number of used features: 39476\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.369966 -> initscore=-0.532361\n",
      "[LightGBM] [Info] Start training from score -0.532361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\anyaa\\AppData\\Roaming\\Python\\Python313\\site-packages\\lightgbm\\basic.py:1238: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning(\"Converting data to scipy sparse matrix.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8203660195375294\n",
      "F1: 0.7503394228951484\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "model = LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=63,\n",
    "    colsample_bytree=0.8,\n",
    "    max_depth=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_final_train, y_train)\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "y_pred = model.predict(X_final_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1:\", f1_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3d95a28-3433-4e52-94fd-fd930653b0ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cross_validate\n\u001b[1;32m----> 3\u001b[0m scores \u001b[38;5;241m=\u001b[39m cross_validate(\n\u001b[0;32m      4\u001b[0m     model,\n\u001b[0;32m      5\u001b[0m     X_final_train,\n\u001b[0;32m      6\u001b[0m     y_train,\n\u001b[0;32m      7\u001b[0m     cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m      8\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     return_train_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     10\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain F1:\u001b[39m\u001b[38;5;124m\"\u001b[39m, scores[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmean())\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation F1:\u001b[39m\u001b[38;5;124m\"\u001b[39m, scores[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmean())\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:411\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    410\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 411\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    412\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    413\u001b[0m         clone(estimator),\n\u001b[0;32m    414\u001b[0m         X,\n\u001b[0;32m    415\u001b[0m         y,\n\u001b[0;32m    416\u001b[0m         scorer\u001b[38;5;241m=\u001b[39mscorers,\n\u001b[0;32m    417\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    418\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    419\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    420\u001b[0m         parameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    421\u001b[0m         fit_params\u001b[38;5;241m=\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit,\n\u001b[0;32m    422\u001b[0m         score_params\u001b[38;5;241m=\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mscorer\u001b[38;5;241m.\u001b[39mscore,\n\u001b[0;32m    423\u001b[0m         return_train_score\u001b[38;5;241m=\u001b[39mreturn_train_score,\n\u001b[0;32m    424\u001b[0m         return_times\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    425\u001b[0m         return_estimator\u001b[38;5;241m=\u001b[39mreturn_estimator,\n\u001b[0;32m    426\u001b[0m         error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[0;32m    427\u001b[0m     )\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[0;32m    429\u001b[0m )\n\u001b[0;32m    431\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    433\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scores = cross_validate(\n",
    "    model,\n",
    "    X_final_train,\n",
    "    y_train,\n",
    "    cv=5,\n",
    "    scoring=\"f1\",\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Train F1:\", scores[\"train_score\"].mean())\n",
    "print(\"Validation F1:\", scores[\"test_score\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372e914d-db97-4455-93c8-9dae5a5e574c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(C=1.2,solver=\"saga\",max_iter=2200,class_weight=\"balanced\")\n",
    "model.fit(X_final_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f96d50-bc78-4c7c-9b43-e6d6c22f060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "\n",
    "X_final_train = imputer.fit_transform(X_final_train)\n",
    "X_final_test = imputer.transform(X_final_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f249efaf-5c52-4758-8a48-3cf13b849133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "y_pred = model.predict(X_final_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1:\", f1_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fa1100-46f3-48b0-b534-6c977bb4f54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "feature_importance = np.abs(model.coef_[0])\n",
    "for col, imp in zip(numeric_cols, feature_importance[-len(numeric_cols):]):\n",
    "    print(col, imp)\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "tfidf_weights = feature_importance[:len(feature_names)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed40f5ed-c58a-4100-821c-03bca3e7bb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scores = cross_validate(\n",
    "    model,\n",
    "    X_final_train,\n",
    "    y_train,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"Train F1:\", scores['train_score'].mean())\n",
    "print(\"Val F1:\", scores['test_score'].mean())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
