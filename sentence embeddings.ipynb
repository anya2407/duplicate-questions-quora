{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5d0f6d8-645e-476a-b126-aa107cd105fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              0\n",
       "qid1            0\n",
       "qid2            0\n",
       "question1       0\n",
       "question2       0\n",
       "is_duplicate    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r\"C:\\Users\\anyaa\\Documents\\nlp\\quora\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "from scipy.sparse import hstack,vstack\n",
    "import contractions\n",
    "df=pd.read_csv(\"questions.csv\")\n",
    "df.head(3)\n",
    "df=df.dropna()\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e53d4508-d1a0-4736-8a65-0e38db5dd68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase (vectorized → fast)\n",
    "df['question1'] = df['question1'].str.lower()\n",
    "df['question2'] = df['question2'].str.lower()\n",
    "# Remove special characters (vectorized → fast)\n",
    "df['question1'] = df['question1'].str.replace(r\"[^a-zA-Z0-9\\s]\", \" \", regex=True)\n",
    "df['question2'] = df['question2'].str.replace(r\"[^a-zA-Z0-9?!\\s]\", \" \", regex=True)\n",
    "# Remove extra spaces (vectorized → fast)\n",
    "df['question1'] = df['question1'].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "df['question2'] = df['question2'].str.replace(r\"\\s+\", \" \", regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c21dcc46-2633-4b6e-b271-2538ea74d860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[['question1','question2']],\n",
    "    df['is_duplicate'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39d3184d-caa7-453b-9fe4-ab12be967130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a76cc015de34b14b2bacc566e4fbc24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anyaa\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\anyaa\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "010d6f4affdf4a90b39418f125402c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c9b07a608d4134bfef652b21b9678a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06588024db75418bb4096eea32abe5bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d17580139d0f4f4cbc7c66271cc8b0f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7303cf565894dd78b95585cb4461b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3109a55473d4286add13a27f2b34306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "666e9cdb20d94fde86f5b6d3f565de21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22bc22c26d1a4da6b33854cb0bf89570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28011a51049d41cf9e5e5e581cf0cdcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f88e242fa44fc78f48a8fb27ce4628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2ef112d0e24ac0ae573823422f973d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "311438e3c663454d986b3873ece79003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f14280c6e840f3801be8458cd56114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "st_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "q1_emb_train = st_model.encode(\n",
    "    X_train[\"question1\"].tolist(),\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "q2_emb_train = st_model.encode(\n",
    "    X_train[\"question2\"].tolist(),\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0e3001c-5248-4303-b36f-c8e4edcc9d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "trans_cos_train = (\n",
    "    np.sum(q1_emb_train * q2_emb_train, axis=1) /\n",
    "    (norm(q1_emb_train, axis=1) * norm(q2_emb_train, axis=1) + 1e-9)\n",
    ")\n",
    "X_train[\"transformer_cosine\"] = trans_cos_train\n",
    "train_dist = norm(q1_emb_train - q2_emb_train, axis=1)\n",
    "X_train[\"transformer_cosine_dist\"] = train_dist\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "X_final = csr_matrix(np.hstack([q1_emb_train , q2_emb_train]))\n",
    "# Precompute char lengths once\n",
    "l1 = X_train['question1'].str.len()\n",
    "l2 = X_train['question2'].str.len()\n",
    "\n",
    "# Assign\n",
    "X_train['lencq1'] = l1\n",
    "X_train['lencq2'] = l2\n",
    "X_train['lendiff'] = l1 - l2\n",
    "\n",
    "# Word lengths\n",
    "X_train['lenq1'] = X_train['question1'].str.split().str.len()\n",
    "X_train['lenq2'] = X_train['question2'].str.split().str.len()\n",
    "\n",
    "#common words\n",
    "q1_words = X_train['question1'].str.split()\n",
    "q2_words = X_train['question2'].str.split()\n",
    "X_train['common_word_count'] = [\n",
    "    len(set(a) & set(b)) for a,b in zip(q1_words, q2_words)\n",
    "]\n",
    "#jaccard similarity\n",
    "def jaccard(q1, q2):\n",
    "    w1 = set(str(q1).split())\n",
    "    w2 = set(str(q2).split())\n",
    "    if len(w1.union(w2)) == 0:\n",
    "        return 0\n",
    "    return len(w1.intersection(w2)) / len(w1.union(w2))\n",
    "\n",
    "X_train['jaccard'] = [\n",
    "    len(set(a.split()) & set(b.split())) /\n",
    "    len(set(a.split()) | set(b.split()))\n",
    "    if len(set(a.split()) | set(b.split())) > 0 else 0\n",
    "    for a,b in zip(X_train['question1'], X_train['question2'])\n",
    "]\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "numeric_cols = ['lenq1','lenq2','lencq1','lencq2','lendiff','common_word_count','jaccard','transformer_cosine',\"transformer_cosine_dist\"]\n",
    "num_train = csr_matrix(X_train[numeric_cols].to_numpy())\n",
    "\n",
    "X_final_train = hstack([X_final, num_train])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bedbfda-fc53-4a3e-b719-21d61885d323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "902624a880ea4a74ab0333667b7fc388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q1_emb_test = st_model.encode(\n",
    "    X_test[\"question1\"].tolist(),\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "q2_emb_test = st_model.encode(\n",
    "    X_test[\"question2\"].tolist(),\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e50be97-80b5-42e8-970c-6549912980a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_test['lenq1'] = X_test['question1'].str.split().str.len()\n",
    "X_test['lenq2'] = X_test['question2'].str.split().str.len()\n",
    "\n",
    "X_test['lencq1']=X_test['question1'].str.len()\n",
    "X_test['lencq2']=X_test['question2'].str.len()\n",
    "X_test['lendiff']=X_test['question1'].str.len()-X_test['question2'].str.len()\n",
    "def common_words(q1, q2):\n",
    "    w1 = set(str(q1).lower().split())\n",
    "    w2 = set(str(q2).lower().split())\n",
    "    return len(w1.intersection(w2))\n",
    "X_test['common_word_count'] = [\n",
    "    len(set(a.split()) & set(b.split()))\n",
    "    for a,b in zip(X_test['question1'], X_test['question2'])\n",
    "]\n",
    "\n",
    "def jaccard(q1, q2):\n",
    "    w1 = set(str(q1).lower().split())\n",
    "    w2 = set(str(q2).lower().split())\n",
    "    if len(w1.union(w2)) == 0:\n",
    "        return 0\n",
    "    return len(w1.intersection(w2)) / len(w1.union(w2))\n",
    "X_test['jaccard'] = X_test.apply(\n",
    "    lambda row: jaccard(row['question1'], row['question2']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "trans_cos_test = (\n",
    "    np.sum(q1_emb_test * q2_emb_test, axis=1) /\n",
    "    (norm(q1_emb_test, axis=1) * norm(q2_emb_test, axis=1) + 1e-9)\n",
    ")\n",
    "\n",
    "X_test[\"transformer_cosine\"] = trans_cos_test\n",
    "test_dist = norm(q1_emb_test - q2_emb_test, axis=1)\n",
    "X_test[\"transformer_cosine_dist\"] = test_dist\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "X_final_t = csr_matrix(np.hstack([q1_emb_test , q2_emb_test]))\n",
    "\n",
    "\n",
    "numeric_cols = [\n",
    "    'lenq1',\n",
    "    'lenq2',\n",
    "    'lencq1','lencq2','lendiff',\n",
    "    'common_word_count',\n",
    "    'jaccard',\n",
    "    'transformer_cosine',\n",
    "    'transformer_cosine_dist'\n",
    "]\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "num_test_sparse = csr_matrix(X_test[numeric_cols].values)\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "X_final_test = hstack([X_final_t, num_test_sparse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85e89a69-c68b-4334-a595-a2cee3baf47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 119676, number of negative: 203802\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.704054 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 197516\n",
      "[LightGBM] [Info] Number of data points in the train set: 323478, number of used features: 777\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.369966 -> initscore=-0.532361\n",
      "[LightGBM] [Info] Start training from score -0.532361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8683689872635093\n",
      "F1: 0.8239127917555787\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "model = LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=63,\n",
    "    colsample_bytree=0.8,\n",
    "    max_depth=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_final_train, y_train)\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "y_pred = model.predict(X_final_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1:\", f1_score(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
